{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading The dataset**\n",
        "\n",
        "you need to download the images from the dataset,The excel file provided in the labeled dataset contains the images paths and thier labels. make sure that the images are downloaded into the following directory (/content/Data)"
      ],
      "metadata": {
        "id": "7u8SjUW2K0_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preprocessing and augmentation**\n",
        "\n",
        "The excel file provided in the labeled dataset contains the images paths and thier labels."
      ],
      "metadata": {
        "id": "hkT7RiAULjd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Data"
      ],
      "metadata": {
        "id": "Fj3rfelMPnD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WL_Wn20Hp-6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from shutil import copyfile\n",
        "\n",
        "def organize_images_by_label(excel_path, image_column, label_column, output_dir):\n",
        "    # Read the Excel file\n",
        "    data = pd.read_excel(excel_path)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Iterate through rows and copy images to label-specific folders\n",
        "    for index, row in data.iterrows():\n",
        "        try:\n",
        "            image_path = str(row[image_column])  # Convert to string\n",
        "            label = str(row[label_column])  # Convert to string\n",
        "\n",
        "            # Skip rows where image path or label is not a valid string\n",
        "            if not image_path.strip() or not label.strip():\n",
        "                print(f\"Skipping row {index + 2} due to empty image path or label.\")\n",
        "                continue\n",
        "\n",
        "            # Create label folder if it doesn't exist\n",
        "            label_folder = os.path.join(output_dir, label)\n",
        "            if not os.path.exists(label_folder):\n",
        "                os.makedirs(label_folder)\n",
        "\n",
        "            # Copy image to label folder\n",
        "            _, image_filename = os.path.split(image_path)\n",
        "            destination_path = os.path.join(label_folder, image_filename)\n",
        "\n",
        "            # Check if destination image already exists, and if not, copy\n",
        "            if not os.path.exists(destination_path):\n",
        "                copyfile(image_path, destination_path)\n",
        "            else:\n",
        "                print(f\"Warning: Image '{image_filename}' already exists in '{label}' folder.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index + 2}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace these with your actual file paths and column names\n",
        "    excel_file_path = '/content/Labels.xlsx'\n",
        "    image_column_name = 'image_path'\n",
        "    label_column_name = 'Best_DET'\n",
        "    output_directory = '/content/new_data/data'\n",
        "\n",
        "    organize_images_by_label(excel_file_path, image_column_name, label_column_name, output_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLKa0ooeOYqq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_folders(root_folder):\n",
        "    total_images = 0\n",
        "\n",
        "    # Iterate through subfolders\n",
        "    for folder_name in os.listdir(root_folder):\n",
        "        folder_path = os.path.join(root_folder, folder_name)\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Count images in the subfolder\n",
        "            images_in_folder = len([f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))])\n",
        "\n",
        "            print(f\"Folder '{folder_name}': {images_in_folder} images\")\n",
        "\n",
        "            # Update the total count\n",
        "            total_images += images_in_folder\n",
        "\n",
        "    print(f\"\\nTotal number of images in all folders: {total_images}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with the path to your root folder\n",
        "    root_folder_path = '/content/new_data/data'\n",
        "\n",
        "    count_images_in_folders(root_folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following cells should be run 8 times, one time for each class to generate more data samples. just change the name of the detector in the path.**\n",
        "\n",
        "for example:\n",
        "change (\"/content/new_data/data/FTRCNN\") to (\"/content/new_data/data/DETR\") and so on for each class."
      ],
      "metadata": {
        "id": "tWL7_d-4TESz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9hxNXmZFwwu"
      },
      "outputs": [],
      "source": [
        "pip install Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS3NiNEDGEDX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def count_images_in_folder(folder_path):\n",
        "    # Ensure the folder path exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder '{folder_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Get a list of all files in the folder\n",
        "    all_files = os.listdir(folder_path)\n",
        "\n",
        "    # Filter only the image files (assuming JPEG format for simplicity)\n",
        "    image_files = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg'))]\n",
        "\n",
        "    # Count the number of image files\n",
        "    num_images = len(image_files)\n",
        "\n",
        "    print(f\"Number of images in folder '{folder_path}': {num_images}\")\n",
        "\n",
        "# Example usage:\n",
        "folder_path = '/content/new_data/data/FTRCNN'\n",
        "count_images_in_folder(folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ0xlXDWFxUa"
      },
      "outputs": [],
      "source": [
        "import Augmentor\n",
        "p = Augmentor.Pipeline(\"/content/new_data/data/FTRCNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2BYoweHFxXK"
      },
      "outputs": [],
      "source": [
        "p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rHMrIbIFxZ5"
      },
      "outputs": [],
      "source": [
        "p.sample(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXUZwMBDGZR7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_images(source_folder, destination_folder):\n",
        "    # Ensure both source and destination folders exist\n",
        "    if not os.path.exists(source_folder):\n",
        "        print(f\"Error: Source folder '{source_folder}' not found.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(destination_folder):\n",
        "        print(f\"Error: Destination folder '{destination_folder}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Get a list of all files in the source folder\n",
        "    all_files = os.listdir(source_folder)\n",
        "\n",
        "    # Filter only the image files (assuming JPEG format for simplicity)\n",
        "    image_files = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg','.png'))]\n",
        "\n",
        "    # Move each image file to the destination folder\n",
        "    for image_file in image_files:\n",
        "        source_path = os.path.join(source_folder, image_file)\n",
        "        destination_path = os.path.join(destination_folder, image_file)\n",
        "\n",
        "        # Move the file\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "        print(f\"Moved: {image_file}\")\n",
        "\n",
        "    print(\"Image move operation completed.\")\n",
        "\n",
        "# Example usage:\n",
        "source_folder = '/content/new_data/data/FTRCNN/output'\n",
        "destination_folder = '/content/new_data/data/FTRCNN'\n",
        "\n",
        "move_images(source_folder, destination_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG4QSkMZHmAP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def count_images_in_folder(folder_path):\n",
        "    # Ensure the folder path exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder '{folder_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Get a list of all files in the folder\n",
        "    all_files = os.listdir(folder_path)\n",
        "\n",
        "    # Filter only the image files (assuming JPEG format for simplicity)\n",
        "    image_files = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg','png'))]\n",
        "\n",
        "    # Count the number of image files\n",
        "    num_images = len(image_files)\n",
        "\n",
        "    print(f\"Number of images in folder '{folder_path}': {num_images}\")\n",
        "\n",
        "# Example usage:\n",
        "folder_path = '/content/new_data/data/FTRCNN'\n",
        "count_images_in_folder(folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following cells will split the data into training and testing splits to prepare for the training**"
      ],
      "metadata": {
        "id": "vnOrUob4Tua-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeEn80MU8NtU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from shutil import copyfile\n",
        "\n",
        "def split_dataset(input_folder, output_train_folder, output_test_folder, test_size=0.2, random_seed=42):\n",
        "    # Get the list of class folders in the input folder\n",
        "    class_folders = [f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
        "\n",
        "    # Create output folders if they don't exist\n",
        "    os.makedirs(output_train_folder, exist_ok=True)\n",
        "    os.makedirs(output_test_folder, exist_ok=True)\n",
        "\n",
        "    # Loop through each class folder\n",
        "    for class_folder in class_folders:\n",
        "        class_path = os.path.join(input_folder, class_folder)\n",
        "\n",
        "        # Get the list of image files for the current class\n",
        "        image_files = [f for f in os.listdir(class_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "        # Check if there are enough samples for splitting\n",
        "        if len(image_files) < 2:\n",
        "            print(f\"Skipping class '{class_folder}' due to insufficient samples.\")\n",
        "            continue\n",
        "\n",
        "        # Split the image files into training and testing sets\n",
        "        train_files, test_files = train_test_split(image_files, test_size=test_size, random_state=random_seed)\n",
        "\n",
        "        # Copy training images to the output training folder\n",
        "        for train_file in train_files:\n",
        "            input_path = os.path.join(class_path, train_file)\n",
        "            output_path = os.path.join(output_train_folder, class_folder, train_file)\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            copyfile(input_path, output_path)\n",
        "\n",
        "        # Copy testing images to the output testing folder\n",
        "        for test_file in test_files:\n",
        "            input_path = os.path.join(class_path, test_file)\n",
        "            output_path = os.path.join(output_test_folder, class_folder, test_file)\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            copyfile(input_path, output_path)\n",
        "\n",
        "    print(\"Dataset split into training and testing sets.\")\n",
        "\n",
        "# Example usage:\n",
        "input_folder = '/content/new_data/data'\n",
        "output_train_folder = '/content/new_data/data/train'\n",
        "output_test_folder = '/content/new_data/data/valid'\n",
        "\n",
        "split_dataset(input_folder, output_train_folder, output_test_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyhQcew8PuG2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_folders(root_folder):\n",
        "    total_images = 0\n",
        "\n",
        "    # Iterate through subfolders\n",
        "    for folder_name in os.listdir(root_folder):\n",
        "        folder_path = os.path.join(root_folder, folder_name)\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Count images in the subfolder\n",
        "            images_in_folder = len([f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))])\n",
        "\n",
        "            print(f\"Folder '{folder_name}': {images_in_folder} images\")\n",
        "\n",
        "            # Update the total count\n",
        "            total_images += images_in_folder\n",
        "\n",
        "    print(f\"\\nTotal number of images in all folders: {total_images}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with the path to your root folder\n",
        "    root_folder_path = '/content/new_data/data/train'\n",
        "\n",
        "    count_images_in_folders(root_folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZpaZBm-PuM3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_folders(root_folder):\n",
        "    total_images = 0\n",
        "\n",
        "    # Iterate through subfolders\n",
        "    for folder_name in os.listdir(root_folder):\n",
        "        folder_path = os.path.join(root_folder, folder_name)\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Count images in the subfolder\n",
        "            images_in_folder = len([f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))])\n",
        "\n",
        "            print(f\"Folder '{folder_name}': {images_in_folder} images\")\n",
        "\n",
        "            # Update the total count\n",
        "            total_images += images_in_folder\n",
        "\n",
        "    print(f\"\\nTotal number of images in all folders: {total_images}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with the path to your root folder\n",
        "    root_folder_path = '/content/new_data/data/valid'\n",
        "\n",
        "    count_images_in_folders(root_folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training ResNet**"
      ],
      "metadata": {
        "id": "rBhUDeiE0Ucg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "id": "UU0vb5nv0ykY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load data\n",
        "data_dir = '/content/new_data/data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# Load pre-trained ResNet and modify the final layer\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))  # Adjust the final layer\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, criterion, optimizer, num_epochs=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            all_labels = []\n",
        "            all_preds = []\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            epoch_precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "            epoch_recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "            epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f} F1: {epoch_f1:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Fine-tune the model\n",
        "model_ft = train_model(model, criterion, optimizer, num_epochs=15)\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load data\n",
        "data_dir = '/content/new_data/data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# Load pre-trained ResNet and modify the final layer\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))  # Adjust the final layer\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, criterion, optimizer, num_epochs=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            all_labels = []\n",
        "            all_preds = []\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            epoch_precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "            epoch_recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "            epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f} F1: {epoch_f1:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Fine-tune the model\n",
        "model_ft = train_model(model, criterion, optimizer, num_epochs=15)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model_ft.state_dict(), 'fine_tuned_resnet50.pth')\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model_ft.state_dict(), 'fine_tuned_resnet50.pth')\n"
      ],
      "metadata": {
        "id": "7RCOFEBU0JP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source file path and the destination directory path\n",
        "source_file = '/content/Data/resnet_swav_finetuned.pth'\n",
        "destination_dir = '/content'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Construct the full destination file path\n",
        "destination_file = os.path.join(destination_dir, os.path.basename(source_file))\n",
        "\n",
        "# Move the file\n",
        "shutil.move(source_file, destination_file)\n",
        "\n",
        "print(f\"File moved from {source_file} to {destination_file}\")\n"
      ],
      "metadata": {
        "id": "EZQXnMhrB08t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet(SwAV)**"
      ],
      "metadata": {
        "id": "StviYC3RZYMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install timm\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import timm\n",
        "\n",
        "# Define the paths\n",
        "TRAIN_DATA_PATH = '/content/new_data/data/train'\n",
        "VALID_DATA_PATH = '/content/new_data/data/valid'\n",
        "\n",
        "# Define data transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DATA_PATH, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(VALID_DATA_PATH, transform=val_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define the ResNet50 model class with SwAV pre-trained weights\n",
        "class ResNetSwAV(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetSwAV, self).__init__()\n",
        "        # Load ResNet50 model with SwAV pre-trained weights\n",
        "        self.resnet = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
        "        # Modify the final layer to match the number of classes in your dataset\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Initialize the model\n",
        "NUM_CLASSES = len(train_dataset.classes)\n",
        "model = ResNetSwAV(NUM_CLASSES)\n",
        "\n",
        "# Define device, criterion, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training and validation loop\n",
        "EPOCHS = 10\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100. * correct_train / total_train\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss/len(train_loader)}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100. * correct_val / total_val\n",
        "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), 'resnet_swav_finetuned.pth')\n",
        "        print(f'Saved model with validation accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "# Plot training and validation performance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example plot\n",
        "train_accuracies = [train_accuracy]  # Replace with actual training accuracy list\n",
        "val_accuracies = [val_accuracy]  # Replace with actual validation accuracy list\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, EPOCHS + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Performance')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GZP6SIBvAqqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source file path and the destination directory path\n",
        "source_file = '/content/Data/resnet_swav_finetuned.pth'\n",
        "destination_dir = '/content'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Construct the full destination file path\n",
        "destination_file = os.path.join(destination_dir, os.path.basename(source_file))\n",
        "\n",
        "# Move the file\n",
        "shutil.move(source_file, destination_file)\n",
        "\n",
        "print(f\"File moved from {source_file} to {destination_file}\")\n"
      ],
      "metadata": {
        "id": "UcCXqyj9EzCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet(**BT**)"
      ],
      "metadata": {
        "id": "GufA-DwUh_W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "id": "UBKZYcXjn0h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models.resnet import Bottleneck, ResNet\n",
        "\n",
        "# Define the custom ResNetTrunk class and functions as provided\n",
        "class ResNetTrunk(ResNet):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        del self.fc  # remove FC layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_pretrained_url(key):\n",
        "    URL_PREFIX = \"https://github.com/lunit-io/benchmark-ssl-pathology/releases/download/pretrained-weights\"\n",
        "    model_zoo_registry = {\n",
        "        \"BT\": \"bt_rn50_ep200.torch\",\n",
        "        \"MoCoV2\": \"mocov2_rn50_ep200.torch\",\n",
        "        \"SwAV\": \"swav_rn50_ep200.torch\",\n",
        "    }\n",
        "    pretrained_url = f\"{URL_PREFIX}/{model_zoo_registry.get(key)}\"\n",
        "    return pretrained_url\n",
        "\n",
        "\n",
        "def resnet50(pretrained, progress, key, **kwargs):\n",
        "    model = ResNetTrunk(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        pretrained_url = get_pretrained_url(key)\n",
        "        verbose = model.load_state_dict(\n",
        "            torch.hub.load_state_dict_from_url(pretrained_url, progress=progress)\n",
        "        )\n",
        "        print(verbose)\n",
        "    return model\n",
        "\n",
        "# Define the paths\n",
        "TRAIN_DATA_PATH = '/content/new_data/data/train'\n",
        "VALID_DATA_PATH = '/content/new_data/data/valid'\n",
        "\n",
        "# Define data transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(TRAIN_DATA_PATH, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(VALID_DATA_PATH, transform=val_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define the complete model with a classification head\n",
        "class ResNetBTFineTune(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetBTFineTune, self).__init__()\n",
        "        self.trunk = resnet50(pretrained=True, progress=False, key=\"BT\")\n",
        "        self.fc = nn.Linear(2048, num_classes)  # Assuming the output from trunk has 2048 features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.trunk(x)\n",
        "        x = nn.AdaptiveAvgPool2d((1, 1))(x)  # Global Average Pooling\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "NUM_CLASSES = len(train_dataset.classes)\n",
        "model = ResNetBTFineTune(NUM_CLASSES)\n",
        "\n",
        "# Define device, criterion, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training and validation loop\n",
        "EPOCHS = 15\n",
        "best_val_accuracy = 0.0\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100. * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss/len(train_loader)}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100. * correct_val / total_val\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Save the model if validation accuracy improves\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), 'resnet_bt_finetuned.pth')\n",
        "        print(f'Saved model with validation accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "# Plot training and validation performance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, EPOCHS + 1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, EPOCHS + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Performance')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6eCOPxjMGgAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source file path and the destination directory path\n",
        "source_file = '/content/Data/resnet_bt_finetuned.pth'\n",
        "destination_dir = '/content'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Construct the full destination file path\n",
        "destination_file = os.path.join(destination_dir, os.path.basename(source_file))\n",
        "\n",
        "# Move the file\n",
        "shutil.move(source_file, destination_file)\n",
        "\n",
        "print(f\"File moved from {source_file} to {destination_file}\")\n"
      ],
      "metadata": {
        "id": "pcMqAFQ1vI20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}